{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Alamos Authentication Classification\n",
    "\n",
    "The following solution is a meta analysis of classifiers available in __[`scikit-learn`](http://scikit-learn.org/stable/index.html)__ partially based on the following examples:\n",
    " - http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n",
    " - http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\n",
    "***\n",
    "To avoid memory limitations within Python, native commands are used via the `subprocess.Popen` class to download and decompress the archived auth.txt.gz file if necessary and find the population size and probability of successful authentication for sampling.  Through manual examination of the dataset the high success rate of authentication became apparent so with prior knowledge of the statistically correct technique to determine sample size (formula __[here](https://www.surveysystem.com/sample-size-formula.htm)__) given a confidence level and interval a relatively small number of elements was needed.  The original number of records exceed one billion but given the `p * (1 - p)` terms in the formula the samples required for a 99.5% confidence level with a $\\pm$0.5% interval are 3202 elements, only about 0.0003% of the total data and much more practical to train the classifiers with.\n",
    "\n",
    "Given the scope of hyperparameters for most of the classifiers an exhaustive grid search was not within reason so a randomized search over predefined ranges of paramters was chosen for training and validation.  Through development of this notebook two of the classifiers, `SCV` and `GaussianProcessClassifier`, were significantly more resource intensive without being more accurate so their options are commented out and the results not included in the analysis.\n",
    "\n",
    "The timestamps included in the output provide an estimate for how long the commands run on the following system:\n",
    " - i7-3632QM CPU at 2.20 GHz\n",
    " - 16 GB of memory\n",
    " - 512 GB solid state drive\n",
    " - 200 MB internet connection over 5 GHz wireless router\n",
    "<br>\n",
    "\n",
    "Please review the documentation and timestamps before running the notebook (especially the note pertaining to downloading and decompressing the auth.txt.gz file).\n",
    "***\n",
    " - System Requirements\n",
    "    - Operating System = Linux based with \"gunzip\", \"wc\", \"grep\", and \"shuf\" commands available\n",
    "    - Python version  = 2.7.5\n",
    "    - Internet connection with access to https://csr.lanl.gov/data/cyber1/auth.txt.gz if archive not available locally\n",
    "    - ~80 GB free space on partition of local directory notebook is executed in (if auth.txt.gz and auth_decompressed.txt files are not available locally)\n",
    " - Version numbers of included third party packages:\n",
    "    - jupyter - 1.0.0\n",
    "    - numpy - 1.14.5\n",
    "    - pandas - 0.23.3\n",
    "    - requests - 2.6.0\n",
    "    - scikit-learn - 0.20.0\n",
    "    - scipy - 1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "import warnings\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import requests\n",
    "import scipy.stats\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the auth.txt.gz data (description available __[here](https://csr.lanl.gov/data/cyber1/)__) and decompress if either is necessary.  Chunking example to accommodate memory limitations of Python found __[here](https://stackoverflow.com/questions/16694907/how-to-download-large-file-in-python-with-requests-py)__.\n",
    " - This notebook was run without the auth.txt.gz or auth_decompressed.txt present in the local directory, note that over three hours were spent downloading and decompressing the file which can be avoided if they are available before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-14 18:22:23.449408] Downloading https://csr.lanl.gov/data/cyber1/auth.txt.gz if not available\n",
      "[2018-10-14 21:56:31.376566] Decompressing auth.txt.gz if not available\n"
     ]
    }
   ],
   "source": [
    "url = \"https://csr.lanl.gov/data/cyber1/auth.txt.gz\"\n",
    "file_name = url.split(\"/\")[-1]\n",
    "decompressed_file_name = \"auth_decompressed.txt\"\n",
    "\n",
    "print \"[%s] Downloading %s if not available\" \\\n",
    "    % (datetime.datetime.now(), url)\n",
    "\n",
    "# Download the compressed authorization file if not available in the local\n",
    "# directory\n",
    "if not (os.path.exists(file_name) and os.path.exists(decompressed_file_name)):\n",
    "    authorization_file = requests.get(url, stream=True)\n",
    "    with open(file_name, \"wb\") as destination:\n",
    "        for chunk in authorization_file.iter_content(chunk_size=1024): \n",
    "            if chunk:\n",
    "                destination.write(chunk)\n",
    "\n",
    "print \"[%s] Decompressing %s if not available\" \\\n",
    "    % (datetime.datetime.now(), file_name)\n",
    "\n",
    "# Decompress the authorization file if not available in the local directory\n",
    "if not os.path.exists(decompressed_file_name):\n",
    "    with open(\"auth_decompressed.txt\", \"wb\") as decompressed_file:\n",
    "        subprocess.call([\"gunzip\", \"-c\", \"auth.txt.gz\"], stdout=decompressed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the needed sample size given the population, probability of successful authentication, and chosen confidence level and interval.  Formula based on documentation found __[here](https://www.surveysystem.com/sample-size-formula.htm)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-14 22:05:43.929428] Calculating sample size\n",
      "Population = 1051430459\n",
      "Z score = 2.5758293035489004\n",
      "Probability of successful authorization = 0.987787772467\n",
      "Required sample size = 3202\n",
      "Reduction of original data to sample = 99.999695%\n"
     ]
    }
   ],
   "source": [
    "print \"[%s] Calculating sample size\" % datetime.datetime.now()\n",
    "\n",
    "# Get the number of records as the population of data\n",
    "p = subprocess.Popen([\"wc\", \"-l\", decompressed_file_name],\n",
    "                     stdout=subprocess.PIPE)\n",
    "row_count = int(p.stdout.read().split()[0])\n",
    "\n",
    "# Z score for 99.5% confidence level\n",
    "z_score = scipy.stats.norm.ppf(0.995)\n",
    "\n",
    "# Calculate the probability of successful authentication\n",
    "p = subprocess.Popen([\"grep\", \"-c\", \",Success\",\n",
    "                     decompressed_file_name], stdout=subprocess.PIPE)\n",
    "probability_success = (int(p.stdout.read()) + 0.0) / row_count\n",
    "\n",
    "# Confidence interval of 0.5%\n",
    "confidence_interval = 0.005\n",
    "\n",
    "# Initial sample size for infinite population\n",
    "sample_size = z_score ** 2 * probability_success * (1\n",
    "        - probability_success) / confidence_interval ** 2\n",
    "\n",
    "# Correction for finite population\n",
    "sample_size = int(math.ceil(sample_size / (1 + (sample_size - 1)\n",
    "                  / row_count)))\n",
    "\n",
    "sample_reduction = \"{0:.6f}\".format(100 * (1 - (sample_size + 0.0)\n",
    "                                    / row_count))\n",
    "\n",
    "print \"Population = %s\" % row_count\n",
    "print \"Z score = %s\" % z_score\n",
    "print \"Probability of successful authorization = %s\" \\\n",
    "    % probability_success\n",
    "print \"Required sample size = %s\" % sample_size\n",
    "print \"Reduction of original data to sample = %s%%\" % sample_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select elements using a uniform random distribution to collect the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-14 22:10:39.014814] Collecting sample\n"
     ]
    }
   ],
   "source": [
    "print \"[%s] Collecting sample\" % datetime.datetime.now()\n",
    "\n",
    "# Randomly select rows of decompressed file to create sample\n",
    "p = subprocess.Popen([\"shuf\", \"-n\", str(sample_size),\n",
    "                     decompressed_file_name], stdout=subprocess.PIPE)\n",
    "\n",
    "# Convert to two dimensional numpy array\n",
    "sample = numpy.genfromtxt(io.BytesIO(p.stdout.read()), dtype=\"|U100\",\n",
    "                          delimiter=\",\", autostrip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training, validation, and verification sets based on the sample.  75% of the sample is used for a combination of training and validation data, the exact percentages for each are determined at runtime by the randomized parameter search for the classifiers listed below.  The remaining 25% is used for the verification set after the optimal parameters have been found through searching.  Definitions for the training, validation, and verification sets are available __[here](https://machinelearningmastery.com/difference-test-validation-datasets/)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-14 22:14:57.681261] Creating training/validation and verification sets\n"
     ]
    }
   ],
   "source": [
    "print \"[%s] Creating training/validation and verification sets\" \\\n",
    "    % datetime.datetime.now()\n",
    "\n",
    "# Create a mapping to assign integer values to input strings\n",
    "mapping_keys = set(numpy.ndarray.tolist(numpy.ndarray.flatten(sample)))\n",
    "mapping = {k: v for k,v in zip(mapping_keys, range(len(mapping_keys)))}\n",
    "\n",
    "# Map all the string values to integers for classifiers\n",
    "for i in range(sample.shape[0]):\n",
    "    for j in range(sample.shape[1]):\n",
    "        sample[i, j] = mapping[sample[i, j]]\n",
    "\n",
    "# Convert integer unicode strings to integer values\n",
    "sample = sample.astype(int, copy=False)\n",
    "\n",
    "# Allocate which rows of the sample are for training/validation and\n",
    "# which are for verification\n",
    "train_validate_rows = range(0, int(0.75 * sample_size))\n",
    "verification_rows = range(int(0.75 * sample_size), sample_size)\n",
    "\n",
    "# Split sample into train/validate and verify sets.\n",
    "x_train_validate = sample[train_validate_rows, :-1]\n",
    "y_train_validate = sample[train_validate_rows, -1]\n",
    "x_verify = sample[verification_rows, :-1]\n",
    "y_verify = sample[verification_rows, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the classifiers with ranges of options for the various parameters of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-14 22:14:57.927459] Setting up classifier parameter options\n"
     ]
    }
   ],
   "source": [
    "print \"[%s] Setting up classifier parameter options\" \\\n",
    "    % datetime.datetime.now()\n",
    "\n",
    "# Define parameters and associated ranges for each classifier\n",
    "clf_opts = {\n",
    "    \"Nearest Neighbors\": (KNeighborsClassifier(), {\n",
    "        \"n_neighbors\": range(1, 16),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "        \"leaf_size\": range(10, 51, 10),\n",
    "        \"p\": range(1, 3),\n",
    "        }),\n",
    "# Inclusion of SVC and Gaussian Process classifiers greatly increase\n",
    "# the overall training duration without a comparable increase in\n",
    "# validation or verification accuracy so they are only included here\n",
    "# for reference\n",
    "#     \"SVM\": (SVC(), {\n",
    "#         \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"],\n",
    "#         \"degree\": range(1, 6),\n",
    "#         \"gamma\": [i / 10.0 for i in range(1, 51)],\n",
    "#         \"coef0\": [i / 2.0 for i in range(1, 6)],\n",
    "#         \"shrinking\": [True, False],\n",
    "#         \"decision_function_shape\": [\"ovo\", \"ovr\"],\n",
    "#         }),\n",
    "#     \"Gaussian Process\": (GaussianProcessClassifier(), {\n",
    "#         \"kernel\": [\n",
    "#             WhiteKernel(),\n",
    "#             ConstantKernel(),\n",
    "#             RBF(),\n",
    "#             Matern(),\n",
    "#             RationalQuadratic(),\n",
    "#             ExpSineSquared(),\n",
    "#             DotProduct(),\n",
    "#             ],\n",
    "#         \"n_restarts_optimizer\": range(1, 4),\n",
    "#         \"warm_start\": [True, False],\n",
    "#         \"multi_class\": [\"one_vs_rest\", \"one_vs_one\"],\n",
    "#         }),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"splitter\": [\"best\", \"random\"],\n",
    "        \"max_depth\": [\n",
    "            1,\n",
    "            2,\n",
    "            3,\n",
    "            4,\n",
    "            5,\n",
    "            None\n",
    "            ],\n",
    "        \"min_samples_split\": [i / 20.0 for i in range(1, 21)],\n",
    "        \"min_samples_leaf\": [i / 20.0 for i in range(1, 11)],\n",
    "        \"min_weight_fraction_leaf\": [i / 10.0 for i in range(6)],\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "        \"min_impurity_decrease\": [i / 5.0 for i in range(1, 6)],\n",
    "        \"presort\": [True, False],\n",
    "        }),\n",
    "    \"Random Forest\": (RandomForestClassifier(), {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [\n",
    "            1,\n",
    "            2,\n",
    "            3,\n",
    "            4,\n",
    "            5,\n",
    "            None\n",
    "            ],\n",
    "        \"min_samples_split\": [i / 20.0 for i in range(1, 21)],\n",
    "        \"min_samples_leaf\": [i / 20.0 for i in range(1, 11)],\n",
    "        \"min_weight_fraction_leaf\": [i / 10.0 for i in range(6)],\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "        \"min_impurity_decrease\": [i / 5.0 for i in range(1, 6)],\n",
    "        \"bootstrap\": [True, False],\n",
    "        }),\n",
    "    \"Neural Net\": (MLPClassifier(), {\n",
    "        \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "        \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "        \"alpha\": [1.0 / 10 ** i for i in range(3, 9)],\n",
    "        \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "        \"shuffle\": [True, False],\n",
    "        \"momentum\": [i / 5.0 for i in range(6)],\n",
    "        \"validation_fraction\": [i / 10.0 for i in range(6)],\n",
    "        \"beta_1\": [i / 10.0 for i in range(6, 10)],\n",
    "        \"beta_2\": [1 - 1 / 10.0 ** i for i in range(1, 6)],\n",
    "        }),\n",
    "    \"Naive Bayes\": (GaussianNB(), {\"var_smoothing\": [1 / 10.0 ** i\n",
    "                    for i in range(3, 20)]}),\n",
    "    \"QDA\": (QuadraticDiscriminantAnalysis(), {\"reg_param\": [i / 10.0\n",
    "            for i in range(6, 10)]}),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the classifiers and perform a randomized search on the given parameter ranges storing the best validation and verification scores as well as the average time elapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-14 22:14:57.968548] Starting meta analysis of classifiers\n",
      "[2018-10-14 22:14:57.969594] Performing randomized search on QDA classifier\n",
      "[2018-10-14 22:14:58.058093] Performing randomized search on Decision Tree classifier\n",
      "[2018-10-14 22:15:07.247340] Performing randomized search on Naive Bayes classifier\n",
      "[2018-10-14 22:15:07.406846] Performing randomized search on Neural Net classifier\n",
      "[2018-10-14 22:51:33.786657] Performing randomized search on Random Forest classifier\n",
      "[2018-10-14 22:53:14.387423] Performing randomized search on Nearest Neighbors classifier\n"
     ]
    }
   ],
   "source": [
    "print \"[%s] Starting meta analysis of classifiers\" % datetime.datetime.now()\n",
    "\n",
    "# Number of parameter combinations to evaluate for each classifier\n",
    "search_iteration_count = 1000\n",
    "\n",
    "# Maximum number of attempts for randomized search of each classifier\n",
    "max_retry_count = 10\n",
    "\n",
    "# Suppress any warnings from classifiers due to random parameter combinations\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Wrapping call to RandomizedSearchCV prevents invalid parameter combinations\n",
    "# from stopping overall progress through classifiers\n",
    "def perform_randomized_search(param_dist, search_iteration_count,\n",
    "                              retry_count):\n",
    "    try:\n",
    "        random_search = RandomizedSearchCV(clf,\n",
    "                param_distributions=param_dist,\n",
    "                n_iter=search_iteration_count)\n",
    "        random_search.fit(x_train_validate, y_train_validate)\n",
    "        return random_search\n",
    "    except:\n",
    "        retry_count += 1\n",
    "        if retry_count < max_retry_count:\n",
    "            return perform_randomized_search(param_dist,\n",
    "                    search_iteration_count, retry_count)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "# Create lists to store best results for formatting later\n",
    "best_names = list()\n",
    "best_validation_scores = list()\n",
    "best_verification_scores = list()\n",
    "best_elapsed_times = list()\n",
    "\n",
    "# Train classifiers and collect results\n",
    "for (k, v) in clf_opts.iteritems():\n",
    "    start = datetime.datetime.now()\n",
    "    print \"[%s] Performing randomized search on %s classifier\" \\\n",
    "        % (start, k)\n",
    "    clf = v[0]\n",
    "    param_dist = v[1]\n",
    "    random_search = perform_randomized_search(param_dist,\n",
    "            search_iteration_count, 0)\n",
    "    elapsed = (datetime.datetime.now() - start).total_seconds() \\\n",
    "        / search_iteration_count\n",
    "    if random_search:\n",
    "        clf.set_params(**random_search.best_params_)\n",
    "        clf.fit(x_train_validate, y_train_validate)\n",
    "        best_names.append(k)\n",
    "        best_validation_scores.append(random_search.best_score_)\n",
    "        best_verification_scores.append(clf.score(x_verify, y_verify))\n",
    "        best_elapsed_times.append(elapsed)\n",
    "    else:\n",
    "        print \"[%s] Maximum number of randomized search attempts for %s met.\" \\\n",
    "            % (datetime.datetime.now(), k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the classifier results sorted in descending order by verification score and ascending average time elapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-10-14 22:55:57.258555] Formatting results of randomized parameter search\n",
      "\n",
      "                   Validation Score  Verification Score  Average Time (Seconds)\n",
      "Classifier                                                                     \n",
      "Decision Tree              0.992087            0.987516                0.009187\n",
      "Random Forest              0.992087            0.987516                0.100572\n",
      "Nearest Neighbors          0.992503            0.987516                0.162836\n",
      "Neural Net                 0.992087            0.987516                2.185270\n",
      "Naive Bayes                0.987922            0.977528                0.000157\n",
      "QDA                        0.958351            0.955056                0.000085\n"
     ]
    }
   ],
   "source": [
    "print \"[%s] Formatting results of randomized parameter search\\n\" % \\\n",
    "    datetime.datetime.now()\n",
    "\n",
    "# Construct dataframe to sort and display results\n",
    "formatted_results = pandas.DataFrame.from_dict({\n",
    "    \"Classifier\": best_names,\n",
    "    \"Validation Score\": best_validation_scores,\n",
    "    \"Verification Score\": best_verification_scores,\n",
    "    \"Average Time (Seconds)\": best_elapsed_times,\n",
    "    })\n",
    "\n",
    "# Move classifier column to index\n",
    "formatted_results.index = formatted_results.pop(\"Classifier\")\n",
    "formatted_results.index.name = \"Classifier\"\n",
    "\n",
    "# Arrange columns in order for display table\n",
    "formatted_results = formatted_results[[\"Validation Score\",\n",
    "        \"Verification Score\", \"Average Time (Seconds)\"]]\n",
    "\n",
    "# Sort classifiers by descending verification score and ascending average time\n",
    "formatted_results.sort_values(by=[\"Verification Score\",\n",
    "                              \"Average Time (Seconds)\"],\n",
    "                              ascending=[False, True], inplace=True)\n",
    "\n",
    "pandas.set_option('display.width', 120)\n",
    "\n",
    "print formatted_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
